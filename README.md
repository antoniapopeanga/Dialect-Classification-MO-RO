# Dialect Classification: Romanian vs Moldovan  

This project, developed within **NitroNLP**, explores the task of distinguishing between **Romanian Standard** and the **Moldovan dialect** using the **MOROCO corpus**. It combines **linguistic feature analysis** with **machine learning models** (Random Forest, TF-IDF) and **deep learning approaches** (fine-tuned RoBERTa for Romanian).  

## üìå Project Overview  

- **Goal**: Automatically classify text samples as either Romanian or Moldovan dialect.  
- **Datasets**:  
  - [MOROCO](https://github.com/butnaruandrei/MOROCO) ‚Äì news corpus for Romanian & Moldovan dialects.  
  - [SlavicNER](https://github.com/BSoboleva/SlavicNER) ‚Äì used for comparative analysis with Russian texts.  

## ‚öôÔ∏è Data Processing  

1. **Dataframes creation** ‚Äì train, validation, and test splits.  
2. **Minimal preprocessing**:  
   - Lowercasing  
   - Removing punctuation, numbers, and extra spaces  
   - Preserving Named Entity tokens  
3. **Linguistic analysis**:  
   - **Diacritics frequency**: Moldovan shows higher density of **√Æ, »ô, »õ**, with **√Æ** often used inside words.  
   - **POS tagging** (using *Stanza* for Romanian):  
     - Romanian ‚Üí more verbs, higher POS variety  
     - Moldovan ‚Üí more nouns  
   - **Russian influence**: similarity scores (cosine similarity with SlavicNER data):  
     - Romanian ‚Üî Moldovan: **0.99**  
     - Romanian ‚Üî Russian: **-0.0005**  
     - Moldovan ‚Üî Russian: **+0.01** (slightly stronger connection than Romanian).  

## üß™ Classification Approaches  

### 1. **Classical ML with Linguistic Features**  
- **Features**: TF-IDF (up to 15,000 n-grams, range 4‚Äì6), diacritics frequency, √Æ/√¢ ratio, POS tags.  
- **Model**: Random Forest.  
- **Performance**:  
  - Validation accuracy: **88.3%**  
  - Test accuracy: **88.4%**  
  - Misclassifications:  
    - 219 / 2472 Moldovan samples  
    - 466 / 3452 Romanian samples  
- **Top discriminative features**:  
  - Frequency of **»ô** and **»õ**  
  - Words like *moldovenesc* and derivatives  
  - Ratios **√Æ/√¢** and **ƒÉ/a**  
  - POS tags (notably *noun*).  

### 2. **Deep Learning with Transformers**  
- **Model**: Fine-tuned *RoBERT* (Romanian pretrained transformer).  
- **Setup**:  
  - Tokenizer with max length 128  
  - CrossEntropyLoss  
  - Training: 5 epochs, batch size 16  
- **Performance**:  
  - Test accuracy: **94.7%**  
- **Insights**:  
  - Captures subtle linguistic traits without manual feature engineering  
  - Outperforms Random Forest by ~6%  

## üìä Results Comparison  

| Model                  | Accuracy (Test) | Notes |
|------------------------|----------------|-------|
| Random Forest (TF-IDF + features) | 88.4% | More interpretable, highlights discriminative features |
| Fine-tuned RoBERT      | 94.7% | Less interpretable, but significantly better performance |

## ‚úÖ Conclusions  

- **Random Forest + linguistic features**: offers interpretability, highlights discriminative linguistic markers, but misses subtle patterns.  
- **RoBERT transformer**: superior performance, learns deep language representations automatically, but harder to interpret.  
- **Overall**: Transformers are more effective for **dialect classification**, while classical ML aids **explainability**.  

## üöÄ Future Work  

- Extend analysis with **cross-lingual embeddings** (Romanian‚ÄìMoldovan‚ÄìRussian).  
- Apply **explainable AI (XAI)** methods (e.g., SHAP, LIME) to interpret RoBERT predictions.
  
---

üî¨ *This project was developed as part of NitroNLP, focusing on the intersection of linguistic analysis and machine learning for dialect classification.*  
